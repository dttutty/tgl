############################################
# 1ï¸âƒ£ è¿è¡Œç¯å¢ƒä¼˜åŒ–ï¼ˆé«˜æ€§èƒ½è®­ç»ƒæ¨¡å¼ï¼‰
############################################
export CUDA_VISIBLE_DEVICES=2,3
export TORCH_DISTRIBUTED_DEBUG=OFF
export LOG_LEVEL=INFO
export NUMBA_DEBUG=0

# ğŸš€ å…³é”®åŠ é€Ÿé¡¹
export CUDA_LAUNCH_BLOCKING=0
export TORCH_USE_CUDA_DSA=0
export NCCL_DEBUG=WARN 
export TORCH_CUDNN_BENCHMARK=1

# æ˜¾å­˜ç®¡ç†
# export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export PYTORCH_CUDA_ALLOC_CONF=roundup_power2_divisions:2


# NCCL ç¨³å®š + ä½æ—¥å¿—
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
export TORCH_NCCL_BLOCKING_WAIT=0

# CPU çº¿ç¨‹æ§åˆ¶ï¼ˆæ ¹æ®æœºå™¨æ ¸æ•°æ”¹ï¼‰
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
export TOKENIZERS_PARALLELISM=false

############################################
# 2ï¸âƒ£ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
############################################
/home/sqp17/miniconda3/envs/simple_py310/bin/python -O -m torch.distributed.run \
    --nproc_per_node=3 \
    --master_addr=127.0.0.1 \
    --master_port=29500 \
    train_dist.py \
    --dataset LASTFM \
    --config /home/sqp17/Projects/original_tgl/config/dist/TGN.yml \
    --num_gpus 2 \
    --rnd_edim 128 \
    --rnd_ndim 128



############################################
# 1ï¸âƒ£ è¿è¡Œç¯å¢ƒä¼˜åŒ–ï¼ˆé«˜æ€§èƒ½è®­ç»ƒæ¨¡å¼ï¼‰
############################################
export CUDA_VISIBLE_DEVICES=2,3
export TORCH_DISTRIBUTED_DEBUG=OFF
export LOG_LEVEL=INFO
export NUMBA_DEBUG=0

# ğŸš€ å…³é”®åŠ é€Ÿé¡¹
export CUDA_LAUNCH_BLOCKING=0
export TORCH_USE_CUDA_DSA=0
export NCCL_DEBUG=WARN 
export TORCH_CUDNN_BENCHMARK=1

# æ˜¾å­˜ç®¡ç†
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# NCCL ç¨³å®š + ä½æ—¥å¿—
export NCCL_DEBUG=VERSION
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
export TORCH_NCCL_BLOCKING_WAIT=0

# CPU çº¿ç¨‹æ§åˆ¶ï¼ˆæ ¹æ®æœºå™¨æ ¸æ•°æ”¹ï¼‰
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
export TOKENIZERS_PARALLELISM=false
/home/sqp17/miniconda3/envs/simple_py310/bin/python \
    -m viztracer \
    --min_duration 200us \
    --output_file distributed_trace.json \
    --ignore_c_function \
    -m torch.distributed.run \
    --nproc_per_node=3 \
    --master_addr=127.0.0.1 \
    --master_port=29505 \
    train_dist.py \
    --data LASTFM \
    --config /home/sqp17/Projects/original_tgl/config/dist/TGN.yml  \